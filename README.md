
# Hi, I'm Devanshi Agarwal 👋

🌟 Welcome to my GitHub profile! I'm a passionate **Data Engineer** with over **5 years of experience** in building and optimizing scalable data pipelines, ETL processes, and transforming raw data into actionable insights. My expertise spans across **Python**, **AWS**, **SQL**, **Snowflake**, **DBT**, **Airflow**, and more.


## 🚀 Skills & Technologies

I specialize in:

- **Data Engineering**: Designing and maintaining data pipelines
- **Programming Languages**: Python, SQL
- **Cloud Platforms**: AWS (S3, EC2, Lambda, Redshift, Glue)
- **Data Warehousing**: Snowflake, Redshift, BigQuery
- **ETL Tools**: Matillion, Airflow, DBT
- **Data Transformation**: DBT (Data Build Tool), SQL-based transformations
- **Data Integration**: Data pipeline orchestration using Airflow
- **DevOps**: CI/CD with Jenkins, Docker  for data pipelines
- **Big Data Technologies**: PySpark, AWS Glue, and Lambda functions for big data processing


## 💼 Projects

Here are some of the key projects I've worked on:

### 1. **Data Pipeline for Real-time Analytics**  
   - **Tech Used**: AWS (S3, Glue, Lambda), Snowflake, DBT  
   - **Description**: Built a scalable pipeline to process and transform streaming data into Snowflake for real-time analytics using AWS Glue and Lambda. Implemented ETL jobs in DBT for data transformations.

### 2. **Airflow Data Pipeline: S3, Snowflake & DBT**  
   - **Tech Used**: Airflow, Snowflake , AWS(S3), DBT 
   - **Description**: This project demonstrates the use of Apache Airflow for orchestrating data pipelines by integrating with AWS S3, Snowflake, and DBT. It showcases building efficient workflows for tasks such as data extraction, transformation, and loading (ETL), along with advanced Airflow features like DAGs, task dependencies, hooks, XComs, and sensors.
     
### 3. **DBT Pipeline for Travel Data Analytics**  
   - **Tech Used**: DBT, Snowflake  
   - **Description**: Designed data models and transformations using DBT to simplify data pipeline workflows and enhance data quality. Integrated DBT with Snowflake for optimized, automated data processing.

[Click here to view more projects!](https://github.com/devanshiagarwal1034?tab=repositories)

<br>  


## 💻 Tools

<img src="https://github.com/user-attachments/assets/03213c43-315e-4c48-b630-2f4ee3f27895" alt="AWS" width="90"/>
<img src="https://github.com/user-attachments/assets/4798bb46-f7d2-45db-8c47-75370d31a00d" alt="DBT" width="120"/>
<img src="https://github.com/user-attachments/assets/ef4698eb-0ccb-4e04-ac4c-0989760791ba" alt="Snowflake" width="150"/>
<img src="https://github.com/user-attachments/assets/e1d05643-6966-46ab-9152-78fe5d2d2f49" alt="Airflow" width="100"/>
<img src="https://github.com/user-attachments/assets/48f5a4c2-8a21-4cfa-8458-7aebe8e605f3" alt="Airflow" width="70"/>


<br>  

## 🤝 Let's Connect

- [**LinkedIn**](https://www.linkedin.com/in/devanshi-agarwal-23303223a/)
- [**Email**](devanshiagarwal1034@gmail.com)
- [**Personal Website**](https://devanshiec1034.wixsite.com/devanshi-portfolio)


<br>  


## 🌱 Learning & Development

I'm always looking to learn new skills and stay up-to-date with the latest trends in data engineering. Here are some areas I'm currently focusing on:
- **Data Engineering**: Advanced transformations with DBT and PySpark.
- **Automation**: Streamlining CI/CD pipelines for data workflows.


<br>  

## 🏆 Achievements & Certifications

- 🏅 **AWS Certified Cloud Practitioner**  
- 🏅 **Google Certified Data Analyst**  

<br>  

## 📝 Recent Blog Posts & Articles

Check out my recent blog posts and articles:
- [Building Scalable Data Pipelines with AWS](https://medium.com/@devanshiec1034/create-an-end-to-end-data-pipeline-to-ingest-transform-store-analyze-and-visualize-data-using-ec21aa56a769))
- [Use AWS Rekognition for Facial Recognition and Analysis](https://medium.com/@devanshiec1034/use-amazon-rekognition-for-facial-recognition-and-analysis-0c68ca6d579e)

<br>  

### 🌟  Thank you for visiting my GitHub profile!
Feel free to dive into my repositories, explore my projects, and connect with me. Let's collaborate to build innovative data solutions and make an impact in the world of data engineering! 🚀
